{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#  REI Option Project Preprocessing Data\n",
    "## Sat Jan  7 19:05:04 CET 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Document classification\n",
    "\n",
    "Text categorization is one of the most active research areas in NLP. It has a variety of real-world applications such as sentiment analysis, opinion mining, email filtering, etc. Given the current data overflow, especially of textual type, the needs for efficient automated text classification solutions have become more pressing than ever.\n",
    "\n",
    "The most common pipeline for text classification is the vector space representation followed by TF- IDF term weighting. With this approach, each document is viewed as a point in a sparse space where each dimension (or feature) is a unique term in the corpus. The set of unique terms is called the vocabulary. If we assume that there are m documents $\\{d_1,\\dots,d_m\\}$ in the collection and n unique terms $T = \\{t_1,\\dots,t_n\\}$ in the corpus ($T$ being the vocabulary), each document can be represented as a vector of n term weights (i.e., the weights are the coordinates of the document in vocabulary space). A classifier is then trained on the available documents (i.e., on a document-term matrix of dimension $m*n$) and subsequently used for classifying new ones. In this project, we consider the classifier as a black-box, and focus only on improving categorization performance by coming up with better term weights. We will compare two ways of computing these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utils import *\n",
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import unicodedata\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 1 Load data\n",
    "\n",
    "Initially, we should load the data that are contained in the *aclImdb* directory. We will work on a standard, publicly available dataset: aclImdb. This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# change this address to your own.\n",
    "# dataset_path = '/Users/zacharie/Documents/AIC/REI(Option)/aclImdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def load_aclImdb_data(path_train, path_test):\n",
    "    sentences_train = []\n",
    "    train_y = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir('%s/pos/' % path_train)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "       \twith open(ff, 'r') as f:\n",
    "\t    sentences_train.append(f.readline().strip())\n",
    "            train_y.append(1)\n",
    "    os.chdir('%s/neg/' % path_train)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "       \twith open(ff, 'r') as f:\n",
    "            sentences_train.append(f.readline().strip())\n",
    "            train_y.append(0)\n",
    "    os.chdir(currdir)\n",
    "    sentences_test = []\n",
    "    test_y = []\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir('%s/pos/' % path_test)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences_test.append(f.readline().strip())\n",
    "            test_y.append(1)\n",
    "    os.chdir('%s/neg/' % path_test)\n",
    "    for ff in glob.glob(\"*.txt\"):\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences_test.append(f.readline().strip())\n",
    "            test_y.append(0)\n",
    "    os.chdir(currdir)\n",
    "    return sentences_train, sentences_test, train_y, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def load_mrd_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import io\n",
    "    sentences_pos = []\n",
    "    ff = \"../rt-polaritydata/rt_polarity.pos.utf8.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_pos.append(line)\n",
    "    sentences_neg = []\n",
    "    ff = \"../rt-polaritydata/rt_polarity.neg.utf8.txt\"\n",
    "    with io.open(ff, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentences_neg.append(line)\n",
    "    terms_by_doc_train, terms_by_doc_test, terms_by_label_train, terms_by_label_test = train_test_split(\\\n",
    "        sentences_pos+sentences_neg, [1]*len(sentences_pos)+[0]*len(sentences_neg), test_size=0.4, random_state=58)\n",
    "    return terms_by_doc_train, terms_by_doc_test, terms_by_label_train, terms_by_label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# terms_by_doc_train, terms_by_doc_test, terms_by_label_train, terms_by_label_test = load_mrd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# sentences_train, sentences_test, train_y, test_y =  load_aclImdb_data(os.path.join(dataset_path, 'train'), os.path.join(dataset_path, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 2 Data preprocessing\n",
    "\n",
    "Before applying any learning algorithm to the data, it is necessary to apply some preprocessing tasks as shown below:\n",
    "\n",
    "1)  Remove punctuation marks (e.g, . ~ , ~ ? ~ :~ ( ~) ~ [ ~]) and transform all characters to lowercase. This can be done using Python's *NLTK* library (http://www.nltk.org/).\n",
    "\n",
    "2)  Remove stop words. These are words that are filtered out before processing any natural language data. This set of words does not offer information about the content of the document and typically corresponds to a set of commonly used words in any language. For example, in the context of a search engine, suppose that your search query is \"how to categorize documents\". If the search engine tries to find web pages that contain the terms \"how\", \"to\", \"categorize\", \"documents\", it will find many more pages that contain the terms \"how\" and \"to\" than pages that contain information about document categorization. This is happening because the terms \"how\" and \"to\" are  commonly used in the English language.  In the code, nltk.corpus.stopwords.words() can return one list of stop words. We used them to clear data.\n",
    "\n",
    "3)  Stemming all words (see Wikipedia's annotation for *Stemming*: http://en.wikipedia.org/wiki/Stemming, i.e., the process of reducing the words to their word stem or root. For example, a stemming algorithm reduces the words \"fishing\", \"fished\", and \"fisher\" to the root word, \"fish\". We used *Porter's* stemmer, contained also in the NLTK library.\n",
    "\n",
    "4)  Store our preprocessed data to *./dumps/stemmered\\_train.csv* and *./dumps/stemmered\\_test.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def stemmering_sentences(sentences_train, sentences_test, train_y, test_y):\n",
    "    # Remove punctuation, stopword and then stemmering\n",
    "    punctuation=set(string.punctuation)\n",
    "    stemmer=PorterStemmer()\n",
    "    for i in range(len(sentences_train)):\n",
    "        tmp = sentences_train[i]\n",
    "        tmp = unicode(tmp, errors='ignore')\n",
    "        doc=[stemmer.stem(word) for word in nltk.word_tokenize(tmp) if (word not in punctuation) and (word not in nltk.corpus.stopwords.words('english'))]\n",
    "        sentences_train[i]=doc\n",
    "    df_train = {'features':sentences_train, 'labels':train_y}\n",
    "    df_train = pd.DataFrame(df_train)\n",
    "    df_train.to_csv('dumps/stemmered_train_mrd.csv', encoding='utf8', index=False)\n",
    "    print \"writing stemmered sentences_train in ./dumps/stemmered_train.csv file is done.\"\n",
    "\n",
    "    for i in range(len(sentences_test)):\n",
    "        tmp = sentences_test[i]\n",
    "        tmp = unicode(tmp, errors='ignore')\n",
    "        doc=[stemmer.stem(word) for word in nltk.word_tokenize(tmp) if (word not in punctuation) and (word not in nltk.corpus.stopwords.words('english'))]\n",
    "        sentences_test[i]=doc\n",
    "    df_test = {'features': sentences_test, 'labels':test_y}\n",
    "    df_test = pd.DataFrame(df_test)\n",
    "    df_test.to_csv('dumps/stemmered_test_mrd.csv', encoding='utf8', index=False)\n",
    "    print \"writing stemmered sentences_test in dumps/stemmered_test.csv is done.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# stemmering_sentences(sentences_train, sentences_test, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def stemmering_sentences_mrd(sentences_train, sentences_test, train_y, test_y):\n",
    "    new_train_X = []\n",
    "    new_train_y = []\n",
    "    # Remove punctuation, stopword and then stemmering\n",
    "    punctuation=set(string.punctuation)\n",
    "    stemmer=PorterStemmer()\n",
    "    for i in range(len(sentences_train)):\n",
    "        tmp = sentences_train[i]\n",
    "        # tmp = unicode(tmp, errors='ignore')\n",
    "        doc=[stemmer.stem(word) for word in nltk.word_tokenize(tmp) if (word not in punctuation) and (word not in nltk.corpus.stopwords.words('english'))]\n",
    "        if len(doc)>=5:\n",
    "            new_train_X.append(doc)\n",
    "            new_train_y.append(train_y[i])\n",
    "    df_train = {'features':new_train_X, 'labels':new_train_y}\n",
    "    df_train = pd.DataFrame(df_train)\n",
    "    df_train.to_csv('dumps/stemmered_train_mrd.csv', encoding='utf8', index=False)\n",
    "    print \"writing stemmered sentences_train in ./dumps/stemmered_train_mrd.csv file is done.\"\n",
    "\n",
    "    new_test_X = []\n",
    "    new_test_y =[]\n",
    "    for i in range(len(sentences_test)):\n",
    "        tmp = sentences_test[i]\n",
    "        # tmp = unicode(tmp, errors='ignore')\n",
    "        doc=[stemmer.stem(word) for word in nltk.word_tokenize(tmp) if (word not in punctuation) and (word not in nltk.corpus.stopwords.words('english'))]\n",
    "        if len(doc)>=5:\n",
    "            new_test_X.append(doc)\n",
    "            new_test_y.append(test_y[i])\n",
    "    df_test = {'features': new_test_X, 'labels':new_test_y}\n",
    "    df_test = pd.DataFrame(df_test)\n",
    "    df_test.to_csv('dumps/stemmered_test_mrd.csv', encoding='utf8', index=False)\n",
    "    print \"writing stemmered sentences_test in dumps/stemmered_test_mrd.csv is done.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# stemmering_sentences_mrd(terms_by_doc_train, terms_by_doc_test, terms_by_label_train, terms_by_label_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 3 Reload clean data to pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# reload necessary data set to pandas DataFrame.\n",
    "train_df = pd.read_csv('dumps/stemmered_train_mrd.csv', encoding='utf8')\n",
    "test_df = pd.read_csv('dumps/stemmered_test_mrd.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing terms from training documents as list of lists\n",
      "min, max and average number of terms per document: 5 39 11\n"
     ]
    }
   ],
   "source": [
    "print \"Storing terms from training documents as list of lists\"\n",
    "terms_by_doc_train = [document.rstrip(']\"').lstrip('\"[').split(\", \") for document in train_df.ix[:,0]]\n",
    "terms_by_label_train = train_df.ix[:, 1]\n",
    "n_terms_per_doc = [len(terms) for terms in terms_by_doc_train]\n",
    "print \"min, max and average number of terms per document:\", min(n_terms_per_doc), max(n_terms_per_doc), sum(n_terms_per_doc)/len(n_terms_per_doc)\n",
    "# print terms_by_doc_train[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing terms from test documents as list of lists\n",
      "min, max and average number of terms per document: 5 32 11\n"
     ]
    }
   ],
   "source": [
    "print \"Storing terms from test documents as list of lists\"\n",
    "terms_by_doc_test = [document.rstrip(']\"').lstrip('\"[').split(\", \") for document in test_df.ix[:,0]]\n",
    "terms_by_label_test = test_df.ix[:, 1]\n",
    "n_terms_per_doc = [len(terms) for terms in terms_by_doc_test]\n",
    "print \"min, max and average number of terms per document:\", min(n_terms_per_doc), max(n_terms_per_doc), sum(n_terms_per_doc)/len(n_terms_per_doc)\n",
    "# print terms_by_doc_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# terms_by_doc_train, _, terms_by_label_train, _ = train_test_split(terms_by_doc_train, terms_by_label_train, test_size=0.7, random_state=42)\n",
    "\n",
    "# terms_by_doc_test, _, terms_by_label_test, _ = train_test_split(terms_by_doc_test, terms_by_label_test, test_size=0.97, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average number of terms: 11\n",
      "the number of unique terms: 11403\n"
     ]
    }
   ],
   "source": [
    "# Store all terms in list\n",
    "all_terms = [terms for sublist in terms_by_doc_train for terms in sublist]\n",
    "# Compute average number of terms\n",
    "avg_len = sum(n_terms_per_doc)/len(n_terms_per_doc)\n",
    "print \"the average number of terms:\", avg_len\n",
    "# Find unique terms\n",
    "all_unique_terms = list(set(all_terms))\n",
    "print \"the number of unique terms:\", len(all_unique_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 4 TF-IDF and TW-IDF Representations\n",
    "The text data (i.e., all the possible documents-reviews) should be transformed to a format that will be used in the learning (i.e., classification) task. As we described above, the data will be represented by the Document-Term matrix, where the rows correspond to the different documents of the collection (i.e., reviews, comments or abstracts) and the columns to the features, which in our case are the different terms (i.e., words). Here, we are interested to find relevant weighting criteria for the Document-Term matrix, i.e., assign a relevance score $w_{ij}$ to each term $t_j$ for each document $d_i$. In our project, we are going to consider to different score functions, the TF-IDF and the TW-IDF.\n",
    "\n",
    "### 4.1 TF-IDF\n",
    "\n",
    "A given document $d$ loads on each dimension of the feature space (each term $t$) according to the following formula, known as the pivoted normalization weighting:\n",
    "$$tf-idf(t,d)= \\frac{1+\\ln⁡(1+\\ln(tf(t,d))}{1-b+b \\times \\frac{|d|}{avgdl}} \\times idf(t,D)$$\n",
    "\n",
    "where\n",
    "\n",
    "1. tf(t,d) is the number of times term t appear in document $d$, \n",
    "2. $|d|$ is the length of document d, \n",
    "3. avgdl is the average document length across the corpus, \n",
    "4. $b=0.08$ is a constant predefined parameter, and\n",
    "5. $idf(t,D)=log((m+1)/df(t))$, with $df(t)$ the number of documents in which the term $t$ appears and $m$ is the number of all documents in collection.\n",
    "\n",
    "We can conclude intuitively that frequent words in a document are representative of that document as long as they are not also very frequent at the corpus level. Note that for all the terms that do not appear in d, the weights are null. Since a given document contains only a small fraction of the vocabulary, most of its coordinates in the vector space are null, leading to a very sparse representation, which is a well-known limitation of the vector space model that motivated (among other things) word embeddings(i.e., word2vect).\n",
    "\n",
    "### 4.2 TF-IWF\n",
    "\n",
    "We propose to leverage the graph-of-words representation of a document to derive a new scoring\n",
    "function:\n",
    "$$tw-idf(t,d)= \\frac{tw(t,d)}{1-b+b \\times \\frac{|d|}{avgdl}} \\times idf(t,D)$$\n",
    "where, $tw(t,d)$ is some graph-of-words based score for term $t$ (for the graph-of-words corresponding to document $d$), and $b=0.08$ (the remainder of the equation is the same as for TF-IDF).\n",
    "\n",
    "Various node centrality criteria are intuitively good candidates for tw(t,d):\n",
    "1. Normalized degree centrality:\n",
    "$$degree(node)=  \\frac{|neighbors(node)|}{|\\text{vertices in graph}|-1}$$\n",
    "Note that in its weighted version, degree centrality sums up the weights of the edges incident to the node instead of simply counting the number of incident edges. Keep in mind that both igraph implementations are not normalized.\n",
    "2. Closeness centrality:\n",
    "$$closeness(node)=  \\frac{|\\text{vertices in graph}|-1}{\\sum_{\\text{node}_i \\in graph} dist(node,\\text{node}_i)}$$\n",
    "Closeness centrality is defined as the inverse of the average shortest path distance from the considered node to the other nodes in the graph. As opposed to degree centrality, closeness is a global metric, in that it aggregates information from the entire graph. Note that the igraph implementation has an argument for normalization. If set to True, the function computes exactly the quantity above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Create a TF-IDF and TW-IDF representation for each document in the training set. For TW- IDF, build a graph-of-words for each document (window size of 3) and consider both the weighted and unweighted versions of degree and closeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create IDF dictionary\n",
    "\n",
    "import math\n",
    "# Number of all documents in collection\n",
    "n_doc = len(terms_by_doc_train)\n",
    "# Store DF values in dictionary\n",
    "# Number of documents in which each term appears is the value of this DF dictionary.\n",
    "df = dict(zip(all_unique_terms, [0]*len(all_unique_terms)))\n",
    "for document in terms_by_doc_train:\n",
    "    unique_words = list(set(document))\n",
    "    for word in unique_words:\n",
    "        df[word] += 1\n",
    "# Store IDF values in dictionary idf(d, t) = log [ (1 + n) / 1 + df(d, t) ] + 1.\n",
    "idf = dict()\n",
    "for element in df.keys():\n",
    "    # idf[element] = math.log((float(n_doc)+1)/(1 +df[element])) + 1\n",
    "    idf[element] = math.log10((float(n_doc)+1)/df[element])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 4.2.1 Graph-of-words \n",
    "\n",
    "Graph representations of textual documents have been proposed for more than a decade. Unlike earlier approaches assuming term independence, such as the bag- of-words, graphs-of-words offer an information-rich way of encoding text, by capturing for instance term dependence and term order. More precisely, a graph-of-words is a graph whose vertices represent unique terms in the document and whose edges capture some meaningful syntactic (grammar), semantic (synonymy), or statistical similarity between terms.\n",
    "\n",
    "In this project, we assume that two vertices are linked by an edge if and only if the two words they represent co-occur in text within a sliding window of predetermined fixed size. This is a statistical approach, as it links all co-occurring terms without considering their meaning or function. The underlying assumption (similar to the Markov assumption in time series) is that dependence only exists between words found close to each other. Edges may be assigned integer weights based on the number of co-occurrences, as shown in Figure 1. Similarly, edges can be directed to encode term order, forward edges matching the natural flow of the text.\n",
    "\n",
    "![Caption.](figures/fig1.png)\n",
    "**Figure 1.** Graph-of-words with main core. Node color indicates the highest core a vertex belongs to (i.e., its core number), from 2-core (white) to 6-core (black).\n",
    "\n",
    "The graph was built on document #1938 from the Hulth (2003) data set with **3** words in same window:\n",
    "\n",
    "A **method** for **solution** of **systems** of **linear** **algebraic** **equations** with **m-dimensional** **lambda** **matrices** A **system** of **linear** **algebraic** **equations** with **m-dimensional** **lambda** **matrices** is considered. The **proposed** **method** of searching for the **solution** of this **system** lies in reducing it to a **numerical** **system** of a **special** **kind**.\n",
    "\n",
    "Note: an interactive web application illustrating graphs-of-words can be found in  https://safetyapp.shinyapps.io/GoWvis/, and is very useful to develop a good intuition for the concept.\n",
    "\n",
    "### 4.2.2 K-core\n",
    "\n",
    "The k-core of a graph corresponds to the **maximal connected subgraph** whose vertices are at least of **degree k** within the subgraph.\n",
    "\n",
    "The core number of a vertex is the highest order of a core that contains this vertex.\n",
    "\n",
    "The core of maximum order is called the main core.\n",
    "\n",
    "It corresponds to a fast and good (although not perfect) approximation of the density, or most cohesive connected component of the graph. The set of all the k-cores of a graph (from the 0-core to the main core) forms what is called the k-core decomposition of a graph. The k-core decomposition of a weighted graph can be computed in linearithmic time and linear space using a min-oriented binary heap to retrieve the vertex of lowest degree at each iteration (n in total). In the unweighted case, the algorithm is linear in time.\n",
    "\n",
    "**Usefule note**: Use the *core_dec* function (found in *library.py*). This function returns the main core of the graph (subgraph, igraph obect). For instance, \n",
    "core_dec(graph, weighted = True)[\"main_core\"]\n",
    "returns the subgraph corresponds to the main core of the graph. The names of the vertices of this subgraph can be obtained as a list via executing the following command:\n",
    "core_dec(graph, weighted = True)[\"main_core\"].vs[\"name\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Create a graph-of-words for each training document by using the *terms_to_graph()* function provided in the *library.py* file. This function takes as input a list of ordered terms and the size of sliding window and returns a graph where its edges are weighted based on term co-occurence within a fixed-size sliding window. Because graph-of-words was consistently reported to give superior results, we used a sliding window of size 4. \n",
    "that means 4 in the *terms_to_graph()* function. Loop over all training documents (list of lists of terms) and store the graphs in a list called *all_graphs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a graph-of-words for each training document. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from library import *\n",
    "print \"Creating a graph-of-words for each training document. \\n\"\n",
    "# 4 means that there are merely 3 words in one sliding window. \n",
    "window = 4\n",
    "all_graphs = []\n",
    "for terms in terms_by_doc_train:\n",
    "    all_graphs.append(terms_to_graph(terms, window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# assert checks (should return True)\n",
    "# print  len(terms_by_doc_train)==len(all_graphs)\n",
    "# print  len(set(terms_by_doc_train[0]))==len(all_graphs[0].vs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 4.2.2 K-core\n",
    "\n",
    "The k-core of a graph corresponds to the **maximal connected subgraph** whose vertices are at least of **degree k** within the subgraph.\n",
    "\n",
    "The core number of a vertex is the highest order of a core that contains this vertex.\n",
    "\n",
    "The core of maximum order is called the main core.\n",
    "\n",
    "It corresponds to a fast and good (although not perfect) approximation of the density, or most cohesive connected component of the graph. The set of all the k-cores of a graph (from the 0-core to the main core) forms what is called the k-core decomposition of a graph. The k-core decomposition of a weighted graph can be computed in linearithmic time and linear space using a min-oriented binary heap to retrieve the vertex of lowest degree at each iteration (n in total). In the unweighted case, the algorithm is linear in time.\n",
    "\n",
    "**Usefule note**: Use the *core_dec* function (found in *library.py*). This function returns the main core of the graph (subgraph, igraph obect). For instance, \n",
    "core_dec(graph, weighted = True)[\"main_core\"]\n",
    "returns the subgraph corresponds to the main core of the graph. The names of the vertices of this subgraph can be obtained as a list via executing the following command:\n",
    "core_dec(graph, weighted = True)[\"main_core\"].vs[\"name\"]\n",
    "Loop over \"all_graphs\" and store the main cores in the \"mcs_weighted\" and \"mcs_unweighted\" lists. In this case, you will end up with two lists of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing vector representations of each training document\n",
      "400 documents have been processed\n",
      "800 documents have been processed\n",
      "1200 documents have been processed\n",
      "1600 documents have been processed\n",
      "2000 documents have been processed\n",
      "2400 documents have been processed\n",
      "2800 documents have been processed\n",
      "3200 documents have been processed\n",
      "3600 documents have been processed\n",
      "4000 documents have been processed\n",
      "4400 documents have been processed\n",
      "4800 documents have been processed\n",
      "5200 documents have been processed\n",
      "5600 documents have been processed\n"
     ]
    }
   ],
   "source": [
    "print \"computing vector representations of each training document\"\n",
    "# reminder of formula\n",
    "b = 0.08\n",
    "features_degree = []\n",
    "features_w_degree = []\n",
    "features_closeness = []\n",
    "features_w_closeness = []\n",
    "features_tfidf = []\n",
    "len_all = len(all_unique_terms)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "idf_keys = idf.keys()\n",
    "\n",
    "# print len(all_graphs)\n",
    "# print len(terms_by_doc_train)\n",
    "for i in xrange(len(all_graphs)):\n",
    "    graph = all_graphs[i]\n",
    "    terms_in_doc = terms_by_doc_train[i]\n",
    "    doc_len = len(terms_in_doc)\n",
    "    # Returns zip (node name, degree, weighted degree, closeness, weighted closeness)\n",
    "    my_metrics = compute_node_centrality(graph)\n",
    "\n",
    "    feature_row_degree = [0]*len_all\n",
    "    feature_row_w_degree = [0]*len_all\n",
    "    feature_row_closeness = [0]*len_all\n",
    "    feature_row_w_closeness = [0]*len_all\n",
    "    feature_row_tfidf = [0]*len_all\n",
    "\n",
    "    for term in list(set(terms_in_doc)):\n",
    "        # term here is unique word in specific document terms_by_doc_train[i]\n",
    "        index = all_unique_terms.index(term)\n",
    "        idf_term = idf[term]\n",
    "        denominator = (1-b+(b*(float(doc_len)/avg_len)))\n",
    "        # find current node's degree, w_degree, closeness and w_closeness\n",
    "        metrics_term = [tuple[1:5] for tuple in my_metrics if tuple[0]==term][0]\n",
    "\n",
    "        # store TW-IDF values\n",
    "        feature_row_degree[index] = (float(metrics_term[0])/denominator) * idf_term\n",
    "        feature_row_w_degree[index] = (float(metrics_term[1])/denominator) * idf_term\n",
    "        feature_row_closeness[index] = (float(metrics_term[2])/denominator) * idf_term\n",
    "        feature_row_w_closeness[index] = (float(metrics_term[3])/denominator) * idf_term\n",
    "\n",
    "        # number of occurences of word in document, this can be also calculated when we count IDF.\n",
    "        tf = terms_in_doc.count(term)\n",
    "\n",
    "        # store TF-IDF value\n",
    "        feature_row_tfidf[index] = ((1+math.log1p(1+math.log1p(tf)))/(1-b+(b*(float(doc_len)/avg_len)))) * idf_term\n",
    "        # feature_row_tfidf[index] = ((1+math.log1p(math.log1p(tf)))/(1-b+(b*(float(doc_len)/avg_len)))) * idf_term\n",
    "    features_degree.append(feature_row_degree)\n",
    "    features_w_degree.append(feature_row_w_degree)\n",
    "    features_closeness.append(feature_row_closeness)\n",
    "    features_w_closeness.append(feature_row_w_closeness)\n",
    "    features_tfidf.append(feature_row_tfidf)\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 400 == 0:\n",
    "        print counter, \"documents have been processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# Convert list of lists into array\n",
    "# Documents as rows, unique words as columns (i.e., document-term matrix)\n",
    "training_set_tfidf = np.array(features_tfidf)\n",
    "training_set_degree = np.array(features_degree)\n",
    "training_set_w_degree = np.array(features_w_degree)\n",
    "training_set_closeness = np.array(features_closeness)\n",
    "training_set_w_closeness = np.array(features_w_closeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# Initialize basic SVM\n",
    "classifier_tfidf = svm.LinearSVC()\n",
    "classifier_degree = svm.LinearSVC()\n",
    "classifier_w_degree = svm.LinearSVC()\n",
    "classifier_closeness = svm.LinearSVC()\n",
    "classifier_w_closeness = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_tfidf.fit(training_set_tfidf, terms_by_label_train)\n",
    "classifier_degree.fit(training_set_degree, terms_by_label_train)\n",
    "classifier_w_degree.fit(training_set_w_degree, terms_by_label_train)\n",
    "classifier_closeness.fit(training_set_closeness, terms_by_label_train)\n",
    "classifier_w_closeness.fit(training_set_w_closeness, terms_by_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a graph-of-words for each testing document \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Creating a graph-of-words for each testing document \\n\"\n",
    "window = 4\n",
    "all_graphs_test = []\n",
    "for terms in terms_by_doc_test:\n",
    "    all_graphs_test.append(terms_to_graph(terms,window))\n",
    "# sanity checks (should return True)\n",
    "# print len(terms_by_doc_test)==len(all_graphs_test)\n",
    "# print len(set(terms_by_doc_test[0]))==len(all_graphs_test[0].vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing vector representations of each testing document\n",
      "400 documents have been processed\n",
      "800 documents have been processed\n",
      "1200 documents have been processed\n",
      "1600 documents have been processed\n",
      "2000 documents have been processed\n",
      "2400 documents have been processed\n",
      "2800 documents have been processed\n",
      "3200 documents have been processed\n",
      "3600 documents have been processed\n"
     ]
    }
   ],
   "source": [
    "print \"computing vector representations of each testing document\"\n",
    "# each testing document is represented in the training space only\n",
    "features_degree_test = []\n",
    "features_w_degree_test = []\n",
    "features_closeness_test = []\n",
    "features_w_closeness_test = []\n",
    "features_tfidf_test = []\n",
    "counter = 0\n",
    "\n",
    "for i in xrange(len(all_graphs_test)):\n",
    "    graph = all_graphs_test[i]\n",
    "    # retain only the terms originally present in the training test\n",
    "    terms_in_doc = [term for term in terms_by_doc_test[i] if term in all_unique_terms]\n",
    "    doc_len = len(terms_in_doc)\n",
    "    \n",
    "    # returns node (1) name, (2) degree, (3) weighted degree, (4) closeness, (5) weighted closeness\n",
    "    my_metrics = compute_node_centrality(graph)\n",
    "    \n",
    "    feature_row_degree_test = [0]*len_all\n",
    "    feature_row_w_degree_test = [0]*len_all\n",
    "    feature_row_closeness_test = [0]*len_all\n",
    "    feature_row_w_closeness_test = [0]*len_all\n",
    "    feature_row_tfidf_test = [0]*len_all\n",
    "    \n",
    "    for term in list(set(terms_in_doc)):\n",
    "        index = all_unique_terms.index(term)\n",
    "        # if this term in test data set has never been in training set, we would return 0 as IDF value.\n",
    "        # idf_term = idf.get(term, default=0)\n",
    "        idf_term = idf[term]\n",
    "        \n",
    "        denominator = (1-b+(b*(float(doc_len)/avg_len)))\n",
    "        metrics_term = [tuple[1:5] for tuple in my_metrics if tuple[0]==term][0]\n",
    "        \n",
    "        # store TW-IDF values\n",
    "        feature_row_degree_test[index] = (float(metrics_term[0])/denominator) * idf_term\n",
    "        feature_row_w_degree_test[index] = (float(metrics_term[1])/denominator) * idf_term\n",
    "        feature_row_closeness_test[index] = (float(metrics_term[2])/denominator) * idf_term\n",
    "        feature_row_w_closeness_test[index] = (float(metrics_term[3])/denominator) * idf_term\n",
    "        \n",
    "        # number of occurences of word in document\n",
    "        tf = terms_in_doc.count(term)\n",
    "        \n",
    "        # store TF-IDF value\n",
    "        feature_row_tfidf_test[index] = ((1+math.log1p(1+math.log1p(tf)))/(1-b+(b*(float(doc_len)/avg_len)))) * idf_term\n",
    "        # feature_row_tfidf_test[index] = ((1+math.log1p(math.log1p(tf)))/(1-b+(b*(float(doc_len)/avg_len)))) * idf_term\n",
    "    \n",
    "    features_degree_test.append(feature_row_degree_test)\n",
    "    features_w_degree_test.append(feature_row_w_degree_test)\n",
    "    features_closeness_test.append(feature_row_closeness_test)\n",
    "    features_w_closeness_test.append(feature_row_w_closeness_test)\n",
    "    features_tfidf_test.append(feature_row_tfidf_test)\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 400 == 0:\n",
    "        print counter, \"documents have been processed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Convert list of lists into array\n",
    "# Documents as rows, unique words as columns (i.e., document-term matrix)\n",
    "\n",
    "testing_set_degree = np.array(features_degree_test)\n",
    "testing_set_w_degree = np.array(features_w_degree_test)\n",
    "testing_set_closeness = np.array(features_closeness_test)\n",
    "testing_set_w_closeness = np.array(features_w_closeness_test)\n",
    "testing_set_tfidf = np.array(features_tfidf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions_tfidf = classifier_tfidf.predict(testing_set_tfidf)\n",
    "predictions_degree = classifier_degree.predict(testing_set_degree)\n",
    "predictions_w_degree = classifier_w_degree.predict(testing_set_w_degree)\n",
    "predictions_closeness = classifier_closeness.predict(testing_set_closeness)\n",
    "predictions_w_closeness = classifier_w_closeness.predict(testing_set_w_closeness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy TF-IDF: 0.687838884586\n",
      "Accuracy TW-IDW degree: 0.69584301575\n",
      "Accuracy TW-IDW weighted degree: 0.692744642396\n",
      "Accuracy TW-IDW closeness: 0.698424993545\n",
      "Accuracy TW-IDW weighted closeness: 0.70100697134\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy TF-IDF:\", metrics.accuracy_score(terms_by_label_test,predictions_tfidf)\n",
    "\n",
    "print \"Accuracy TW-IDW degree:\", metrics.accuracy_score(terms_by_label_test,predictions_degree)\n",
    "\n",
    "print \"Accuracy TW-IDW weighted degree:\", metrics.accuracy_score(terms_by_label_test,predictions_w_degree)\n",
    "\n",
    "print \"Accuracy TW-IDW closeness:\", metrics.accuracy_score(terms_by_label_test,predictions_closeness)\n",
    "\n",
    "print \"Accuracy TW-IDW weighted closeness:\", metrics.accuracy_score(terms_by_label_test,predictions_w_closeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hasher = HashingVectorizer(n_features=2**18, stop_words='english', non_negative=True, norm=None, binary=False)\n",
    "vectorizer = Pipeline([('hasher', hasher), ('tf_idf', TfidfTransformer(smooth_idf=False))])\n",
    "    # vectorizer = Pipeline([('hasher', hasher)])\n",
    "X_train = vectorizer.fit_transform([' '.join(term) for term in terms_by_doc_train])\n",
    "X_test = vectorizer.fit_transform([' '.join(term) for term in terms_by_doc_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy TF-IDF: 0.718822618125\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "classifier_original_tfidf = svm.LinearSVC()\n",
    "classifier_original_tfidf.fit(X_train, terms_by_label_train)\n",
    "predictions_original_tfidf = classifier_original_tfidf.predict(X_test)\n",
    "print \"Accuracy TF-IDF:\", metrics.accuracy_score(terms_by_label_test, predictions_original_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "new_presentation.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
